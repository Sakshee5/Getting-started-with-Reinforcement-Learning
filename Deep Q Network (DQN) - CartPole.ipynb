{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CartPole-v1\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Action:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "        \n",
    "    Observation: \n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24°           24°\n",
    "        3\tPole Velocity At Tip      -Inf            Inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the CartPole Enviroment for 1000 episodes without any training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sakshee\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Deep Q Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyModel(state_shape, action_shape):\n",
    "    \"\"\" \n",
    "    The agent maps X-states (we say 'X' because a number of states gets passed in a batch) to Y-actions (2 in this case)\n",
    "    e.g. The neural network output is [.1, .7]\n",
    "    This output are the Q-Values corresponding to a particular state in the batch. 0.1 = q(s, a1) and 0.7 = q(s, a2)  \n",
    "    where a1 = [1, 0] and a2 = [0, 1]\n",
    "    \n",
    "    Neural network input = (batch_size, state_shape)\n",
    "    Neural network output = Dense layer where number of units = number of possible actions (2 in this case of cartpole env)\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    state_shape - (4, ) i.e. the 4 states\n",
    "    action_shape - possible number of actions which are 2 in this case\n",
    "    \n",
    "    RETURNS: Compiled Neural Network\n",
    "    \"\"\"\n",
    "    learning_rate = 0.001\n",
    "    init = tf.keras.initializers.he_uniform()    # neural network weight initializers\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the previously compiled DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, replay_memory, train_model, target_model, done):\n",
    "    \"\"\"\n",
    "    Function to collect training data in the proper format so as to train the previously compiled neural network\n",
    "    \"\"\"\n",
    "    learning_rate = 0.001    # Learning rate\n",
    "    discount_factor = 0.89   # gamma\n",
    "    batch_size = 64 * 2\n",
    "    min_experiences = 200   # minimum experiences to collect before starting to train\n",
    "    \n",
    "    if len(replay_memory) < min_experiences:\n",
    "        return\n",
    "    \n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    \n",
    "    # Replay_memory = [state, action, reward new_state, done] \n",
    "    #               = [[1, 2, 3, 4], [1], [1], [1,2,3,4], True]\n",
    "    # We predict Q values of current state i.e for every experience in mini_batch we have experience[0] as the current state\n",
    "    # We predict Q values of next state i.e experience[3] using target network so as to plug this value into bellman equation\n",
    "    current_states = []\n",
    "    for experience in mini_batch:\n",
    "        current_states.append((experience[0]))  # creating a batch of current states\n",
    "        \n",
    "    current_states = np.array(current_states)   # converting to array since predict function does not accept a list input\n",
    "    current_qs_list = train_model.predict(current_states)  # Q values of states at time t1\n",
    "\n",
    "    new_states = []\n",
    "    for experience in mini_batch:\n",
    "        new_states.append((experience[3]))  # creating a batch of states after a particular current state\n",
    "        \n",
    "    new_states = np.array(new_states)\n",
    "    new_qs_list = target_model.predict(new_states) # Q values of states at time t2\n",
    "\n",
    "    X = []   # list to create training data i.e. batches of states\n",
    "    Y = []   # list to create corresponding Q value labels using Bellman equation\n",
    "    for index, (state, action, reward, new_state, done) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(new_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]    # list of Q values for each sample \n",
    "        # here action is either 0 or 1, we update the q value of the action taken using bellman equation\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q   # bellman equation\n",
    "\n",
    "        X.append(state)\n",
    "        Y.append(current_qs)\n",
    "\n",
    "    train_model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to choose action for every time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(train_model, state, epsilon):\n",
    "    \"\"\"\n",
    "    Pass ONE state to a trained model's predict function to get corresponding Q values. This function is defined for getting\n",
    "    the Q values for greedy Epsilon strategy.\n",
    "    The reshaping is done so that the dimensions of training networks input = dimensions of state being passed\n",
    "    example: state = [ 0.03983203  0.04474036 -0.0341696  -0.00012871]\n",
    "             state.shape = (4, )\n",
    "             state.reshape([1, state.shape[0]]) = array([[ 0.03983203,  0.04474036, -0.0341696 , -0.00012871]])\n",
    "             reshaped shape = (1, 4) \n",
    "             since predict function inputs (batch_size, state_shape)\n",
    "    ARGUMENTS:\n",
    "    model - trained model\n",
    "    state - the state for which we want to predict the Q values corresponding to each action\n",
    "    \n",
    "    RETURNS: predicted Q values for a particular state\n",
    "    \"\"\"\n",
    "     # Implementing Epsilon Greedy Exploration Strategy\n",
    "    random_number = random.uniform(0, 1)\n",
    "    if random_number <= epsilon: \n",
    "        action = env.action_space.sample()  # Explore    \n",
    "    else:\n",
    "        predict = train_model.predict(state.reshape([1, state.shape[0]]))[0]\n",
    "        action = np.argmax(predict)         # Exploit best known action\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function to loop over each time step of each episode for DQN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # setting and getting acquainted with the environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    print(\"Action Space: {}\".format(env.action_space))\n",
    "    print(\"State space: {}\".format(env.observation_space))\n",
    "    state_shape = env.observation_space.shape   # equals value (4, )\n",
    "    action_shape = env.action_space.n           # equals value 2\n",
    "    \n",
    "    # Initializing values for Epsilon-greedy algorithm \n",
    "    epsilon = 1     \n",
    "    max_epsilon = 1 \n",
    "    min_epsilon = 0.01  \n",
    "    decay = 0.01\n",
    "\n",
    "    # Build the Training and Target models\n",
    "    train_model = MyModel(state_shape, action_shape)\n",
    "    target_model = MyModel(state_shape, action_shape)\n",
    "    \n",
    "    # initializing and setting target model parameters\n",
    "    counter_to_update_target_model = 0        # counter\n",
    "    steps_to_update_target_model = 100        # update after every 100 steps\n",
    "    \n",
    "    # setting experience memory parameters\n",
    "    # deque: Queue data structure that allows insert and delete at both ends\n",
    "    replay_memory = deque(maxlen=50_000)      \n",
    "    \n",
    "    # setting other required parameters\n",
    "    max_episodes = 200          # maximum number of episodes to play\n",
    "    rewards_all_episodes = []   # list to keep track of rewards per episode for final visualizations\n",
    "    episode_number = 0          # initialized for visualization purposes\n",
    "    \n",
    "    for episode in range(max_episodes):    # loop to iterate over each episode\n",
    "        episode_number+=1\n",
    "        print('Executing episode:'+ str(episode_number) + ' with epsilon value:'+ str(epsilon))\n",
    "        reward_current_episode = 0         # variable to keep track of rewards for every new episode\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done:     # loop to iterate over each time step of an episode\n",
    "            action = choose_action(train_model, state, epsilon)\n",
    "                \n",
    "            # implement the chosen action and after every action, append the experience into replay memory\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            replay_memory.append([state, action, reward, new_state, done])\n",
    "            \n",
    "            state = new_state\n",
    "            reward_current_episode += reward\n",
    "\n",
    "            # Train the training network using the Bellman Equation\n",
    "            train(env, replay_memory, train_model, target_model, done)\n",
    "            counter_to_update_target_model += 1\n",
    "            \n",
    "            if counter_to_update_target_model == steps_to_update_target_model:\n",
    "                print('Copying main network weights to the target network weights')\n",
    "                target_model.set_weights(train_model.get_weights())\n",
    "                counter_to_update_target_model = 0\n",
    "                break\n",
    "        \n",
    "        # reduce the epsilon value after every episode so as to favor exploitation with time\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "        rewards_all_episodes.append(reward_current_episode)\n",
    "    \n",
    "    # Results Visualizations\n",
    "    rewards_per_twenty_episodes = np.split(np.array(rewards_all_episodes), max_episodes/20)                                                                                      \n",
    "    count = 20\n",
    "    print('Average rewards per twenty episodes')  \n",
    "\n",
    "    for r in rewards_per_twenty_episodes:\n",
    "        print(count,':',str(sum(r/20)))\n",
    "        count+=20\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(2)\n",
      "State space: Box(4,)\n",
      "Executing episode: 1\n",
      "Executing episode: 2\n",
      "Executing episode: 3\n",
      "Executing episode: 4\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 5\n",
      "Executing episode: 6\n",
      "Executing episode: 7\n",
      "Executing episode: 8\n",
      "Executing episode: 9\n",
      "Executing episode: 10\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 11\n",
      "Executing episode: 12\n",
      "Executing episode: 13\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 14\n",
      "Executing episode: 15\n",
      "Executing episode: 16\n",
      "Executing episode: 17\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 18\n",
      "Executing episode: 19\n",
      "Executing episode: 20\n",
      "Executing episode: 21\n",
      "Executing episode: 22\n",
      "Executing episode: 23\n",
      "Executing episode: 24\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 25\n",
      "Executing episode: 26\n",
      "Executing episode: 27\n",
      "Executing episode: 28\n",
      "Executing episode: 29\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 30\n",
      "Executing episode: 31\n",
      "Executing episode: 32\n",
      "Executing episode: 33\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 34\n",
      "Executing episode: 35\n",
      "Executing episode: 36\n",
      "Executing episode: 37\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 38\n",
      "Executing episode: 39\n",
      "Executing episode: 40\n",
      "Executing episode: 41\n",
      "Executing episode: 42\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 43\n",
      "Executing episode: 44\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 45\n",
      "Executing episode: 46\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 47\n",
      "Executing episode: 48\n",
      "Executing episode: 49\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 50\n",
      "Executing episode: 51\n",
      "Executing episode: 52\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 53\n",
      "Executing episode: 54\n",
      "Executing episode: 55\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 56\n",
      "Executing episode: 57\n",
      "Executing episode: 58\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 59\n",
      "Executing episode: 60\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 61\n",
      "Executing episode: 62\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 63\n",
      "Executing episode: 64\n",
      "Executing episode: 65\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 66\n",
      "Executing episode: 67\n",
      "Executing episode: 68\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 69\n",
      "Executing episode: 70\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 71\n",
      "Executing episode: 72\n",
      "Executing episode: 73\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 74\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 75\n",
      "Executing episode: 76\n",
      "Executing episode: 77\n",
      "Executing episode: 78\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 79\n",
      "Executing episode: 80\n",
      "Executing episode: 81\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 82\n",
      "Executing episode: 83\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 84\n",
      "Executing episode: 85\n",
      "Executing episode: 86\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 87\n",
      "Executing episode: 88\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 89\n",
      "Executing episode: 90\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 91\n",
      "Executing episode: 92\n",
      "Copying main network weights to the target network weights\n",
      "Executing episode: 93\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the above result\n",
    "\n",
    "Initially it's taking 4-7 episodes before the weights of target net are updated i.e. 100 (value after which we want to update the weights of Target Network) time steps takes 4-7 episodes. As we reach the 80th episode we see that 100 time steps get covered in just 2-3 episodes which means that our pole is able to balance for more time steps which means it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
