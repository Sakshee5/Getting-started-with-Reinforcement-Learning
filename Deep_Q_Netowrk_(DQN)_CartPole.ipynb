{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Deep Q Netowrk (DQN) - CartPole.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNP-Zm4dK-oM"
      },
      "source": [
        "#### CartPole-v1\n",
        "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQY8-_YjK-oU"
      },
      "source": [
        "    Action:\n",
        "        Type: Discrete(2)\n",
        "        Num\tAction\n",
        "        0\tPush cart to the left\n",
        "        1\tPush cart to the right\n",
        "        \n",
        "    Observation: \n",
        "        Type: Box(4)\n",
        "        Num\tObservation                 Min         Max\n",
        "        0\tCart Position             -4.8            4.8\n",
        "        1\tCart Velocity             -Inf            Inf\n",
        "        2\tPole Angle                 -24°           24°\n",
        "        3\tPole Velocity At Tip      -Inf            Inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCcGJ12jK-oV"
      },
      "source": [
        "### Visualization of the CartPole Enviroment for 1000 episodes without any training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJBYQ5SdK-oV",
        "outputId": "798eb9ca-658c-48f4-a6d0-530ab44eb2f4"
      },
      "source": [
        "import gym\n",
        "env = gym.make('CartPole-v1')\n",
        "env.reset()\n",
        "for _ in range(1000):\n",
        "    env.render()\n",
        "    env.step(env.action_space.sample())\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sakshee\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJJ_G4YaK-oW"
      },
      "source": [
        "### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd6i1xLzK-oX"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from collections import deque\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XAOQIKgK-oX"
      },
      "source": [
        "### Defining the Deep Q Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwdY40GtK-oX"
      },
      "source": [
        "def MyModel(state_shape, action_shape):\n",
        "    \"\"\" \n",
        "    The agent maps X-states (we say 'X' because a number of states gets passed in a batch) to Y-actions(2 in this case)\n",
        "    e.g. The neural network output is [.1, .7]\n",
        "    The highest value 0.7 is the Q-Value thus corresponding action will be it's index. \n",
        "    The index of the highest action (0.7) is action # 1.\n",
        "    \n",
        "    ARGUMENTS:\n",
        "    state_shape - (4, ) i.e. the 4 states\n",
        "    action_shape - possible number of actions which are 2 in this case\n",
        "    \n",
        "    RETURNS: Compiled Neural Network\n",
        "    \"\"\"\n",
        "    learning_rate = 0.001\n",
        "    init = tf.keras.initializers.he_uniform()    # neural network weight initializers\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtwULK_JK-oY"
      },
      "source": [
        "### Function to train the previously compiled DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-MN1oIfK-oY"
      },
      "source": [
        "def train(env, replay_memory, train_model, target_model, done):\n",
        "    \"\"\"\n",
        "    Function to collect training data in the proper format so as to train the previously compiled neural network\n",
        "    \"\"\"\n",
        "    learning_rate = 0.001     # Learning rate\n",
        "    discount_factor = 0.89   # gamma\n",
        "    batch_size = 64 * 2\n",
        "    min_experiences = 200   # minimum experiences to collect before starting to train\n",
        "    \n",
        "    if len(replay_memory) < min_experiences:\n",
        "        return\n",
        "    \n",
        "    mini_batch = random.sample(replay_memory, batch_size)\n",
        "    \n",
        "    # Replay_memory = [state, action, reward new_state, done] \n",
        "    #               = [[1, 2, 3, 4], [1], [1], [1,2,3,4], True]\n",
        "    # We predict Q values of current state i.e for every experience in mini_batch we have experience[0] as the current state\n",
        "    # We predict Q values of next state i.e experience[3] using target network so as to plug this value into bellman equation\n",
        "    current_states = []\n",
        "    for experience in mini_batch:\n",
        "        current_states.append((experience[0]))  # creating a batch of current states\n",
        "    current_states = np.array(current_states)   # converting to array since predict function does not accept a list input\n",
        "    current_qs_list = train_model.predict(current_states)  # Q values of states at time t1\n",
        "\n",
        "    new_current_states = []\n",
        "    for experience in mini_batch:\n",
        "        new_current_states.append((experience[3]))  # creating a batch of states after a particular current state\n",
        "    new_current_states = np.array(new_current_states)\n",
        "    future_qs_list = target_model.predict(new_current_states) # Q values of states at time t2\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for index, (state, action, reward, new_state, done) in enumerate(mini_batch):\n",
        "        if not done:\n",
        "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
        "        else:\n",
        "            max_future_q = reward\n",
        "\n",
        "        current_qs = current_qs_list[index]\n",
        "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q   # bellman equation\n",
        "\n",
        "        X.append(state)\n",
        "        Y.append(current_qs)\n",
        "\n",
        "    train_model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svy6340qK-oZ"
      },
      "source": [
        "### Helper Function to get Q values of a particular state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBbW3y1gK-oa"
      },
      "source": [
        "def get_qs(model, state):\n",
        "    \"\"\"\n",
        "    Pass ONE state to a trained model's predict function to get corresponding Q values. This function is defined for getting\n",
        "    the Q values for greedy Epsilon strategy.\n",
        "    The reshaping is done so that the dimensions of training networks input = dimensions of state being passed\n",
        "    example: state = [ 0.03983203  0.04474036 -0.0341696  -0.00012871]\n",
        "             state.shape = (4, )\n",
        "             state.reshape([1, state.shape[0]]) = array([[ 0.03983203,  0.04474036, -0.0341696 , -0.00012871]])\n",
        "             reshaped shape = (1, 4) \n",
        "             since predict function inputs (batch_size, state_shape)\n",
        "    ARGUMENTS:\n",
        "    model - trained model\n",
        "    state - the state for which we want to predict the Q values corresponding to each action\n",
        "    \n",
        "    RETURNS: predicted Q values for a particular state\n",
        "    \"\"\"\n",
        "    return model.predict(state.reshape([1, state.shape[0]]))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_lxMhLaK-oa"
      },
      "source": [
        "### Main Function to loop over each time step of each episode for DQN training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ2R1a1pK-oa"
      },
      "source": [
        "def main():\n",
        "    \n",
        "    # setting and getting acquainted with the environment\n",
        "    env = gym.make('CartPole-v1')\n",
        "    print(\"Action Space: {}\".format(env.action_space))\n",
        "    print(\"State space: {}\".format(env.observation_space))\n",
        "    state_shape = env.observation_space.shape   # equals value (4, )\n",
        "    action_shape = env.action_space.n           # equals value 2\n",
        "    \n",
        "    # Initializing values for Epsilon-greedy algorithm \n",
        "    epsilon = 1     \n",
        "    max_epsilon = 1 \n",
        "    min_epsilon = 0.01  \n",
        "    decay = 0.01\n",
        "\n",
        "    # Build the Training and Target models\n",
        "    train_model = MyModel(state_shape, action_shape)\n",
        "    target_model = MyModel(state_shape, action_shape)\n",
        "    # initializing and setting target model parameters\n",
        "    steps_to_update_target_model = 0  # counter\n",
        "    update_target_model = 100         # update after every 100 steps\n",
        "    \n",
        "    # setting experience memory parameters\n",
        "    replay_memory = deque(maxlen=50_000)   # deque: Queue data structure that allows insert and delete at both ends\n",
        "    \n",
        "    # setting other required parameters\n",
        "    train_episodes = 200        # maximum number of episodes to play\n",
        "    rewards_all_episodes = []   # list to keep track of rewards after every episode so that we can see average rewards after every 50 - 100 episodes to see how much and if the agent is learning\n",
        "    episode_number = 0\n",
        "    \n",
        "    for episode in range(train_episodes):    # loop to iterate over each episode\n",
        "        episode_number+=1\n",
        "        print('Executing episode:'+ str(episode_number) + ' with epsilon value:'+ str(epsilon))\n",
        "        reward_current_episode = 0           # to keep track of rewards per episode thus initialized to zero after end of a episode\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        \n",
        "        while not done:     # loop to iterate over each step of an episode\n",
        "             # Implementing Epsilon Greedy Exploration Strategy\n",
        "            random_number = random.uniform(0, 1)\n",
        "            if random_number <= epsilon: \n",
        "                action = env.action_space.sample()  # Explore    \n",
        "            else:\n",
        "                predict_maxQ_for_given_state = get_qs(train_model, state).flatten()\n",
        "                action = np.argmax(predict_maxQ_for_given_state)       # Exploit best known action\n",
        "                \n",
        "            # implement the chosen action and after every action, append the experience into replay memory\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            replay_memory.append([state, action, reward, new_state, done])\n",
        "            \n",
        "            state = new_state\n",
        "            reward_current_episode += reward\n",
        "\n",
        "            # Train the training network using the Bellman Equation\n",
        "            train(env, replay_memory, train_model, target_model, done)\n",
        "            steps_to_update_target_model += 1\n",
        "            \n",
        "            if steps_to_update_target_model >= update_target_model:\n",
        "                print('Copying main network weights to the target network weights')\n",
        "                target_model.set_weights(train_model.get_weights())\n",
        "                steps_to_update_target_model = 0\n",
        "                break\n",
        "        \n",
        "        # reduce the epsilon value after every episode so as to favor exploitation with time\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
        "        rewards_all_episodes.append(reward_current_episode)\n",
        "        \n",
        "    rewards_per_twenty_episodes = np.split(np.array(rewards_all_episodes), train_episodes/20)                                                                                      \n",
        "    count = 20\n",
        "    print('Average rewards per twenty episodes')  \n",
        "\n",
        "    for r in rewards_per_twenty_episodes:\n",
        "        print(count,':',str(sum(r/20)))\n",
        "        count+=20\n",
        "        \n",
        "    env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEbauV-wK-ob",
        "outputId": "0a61fdaa-cf7f-45b0-c123-5223c43d1e34"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(2)\n",
            "State space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Executing episode:1 with epsilon value:1\n",
            "Executing episode:2 with epsilon value:1.0\n",
            "Executing episode:3 with epsilon value:0.9901493354116764\n",
            "Executing episode:4 with epsilon value:0.9803966865736877\n",
            "Executing episode:5 with epsilon value:0.970741078213023\n",
            "Executing episode:6 with epsilon value:0.9611815447608\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:7 with epsilon value:0.9517171302557069\n",
            "Executing episode:8 with epsilon value:0.9423468882484062\n",
            "Executing episode:9 with epsilon value:0.9330698817068888\n",
            "Executing episode:10 with epsilon value:0.9238851829227694\n",
            "Executing episode:11 with epsilon value:0.9147918734185159\n",
            "Executing episode:12 with epsilon value:0.9057890438555999\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:13 with epsilon value:0.896875793943563\n",
            "Executing episode:14 with epsilon value:0.888051232349986\n",
            "Executing episode:15 with epsilon value:0.8793144766113556\n",
            "Executing episode:16 with epsilon value:0.8706646530448178\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:17 with epsilon value:0.8621008966608072\n",
            "Executing episode:18 with epsilon value:0.8536223510765493\n",
            "Executing episode:19 with epsilon value:0.8452281684304199\n",
            "Executing episode:20 with epsilon value:0.8369175092971592\n",
            "Executing episode:21 with epsilon value:0.8286895426039287\n",
            "Executing episode:22 with epsilon value:0.820543445547202\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:23 with epsilon value:0.8124784035104852\n",
            "Executing episode:24 with epsilon value:0.8044936099828537\n",
            "Executing episode:25 with epsilon value:0.7965882664783007\n",
            "Executing episode:26 with epsilon value:0.7887615824558879\n",
            "Executing episode:27 with epsilon value:0.7810127752406908\n",
            "Executing episode:28 with epsilon value:0.7733410699455306\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:29 with epsilon value:0.7657456993934846\n",
            "Executing episode:30 with epsilon value:0.7582259040411682\n",
            "Executing episode:31 with epsilon value:0.7507809319027796\n",
            "Executing episode:32 with epsilon value:0.7434100384749007\n",
            "Executing episode:33 with epsilon value:0.7361124866620463\n",
            "Executing episode:34 with epsilon value:0.7288875467029541\n",
            "Executing episode:35 with epsilon value:0.7217344960976069\n",
            "Executing episode:36 with epsilon value:0.7146526195349836\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:37 with epsilon value:0.7076412088215263\n",
            "Executing episode:38 with epsilon value:0.7006995628103208\n",
            "Executing episode:39 with epsilon value:0.6938269873309811\n",
            "Executing episode:40 with epsilon value:0.6870227951202322\n",
            "Executing episode:41 with epsilon value:0.680286305753183\n",
            "Executing episode:42 with epsilon value:0.6736168455752829\n",
            "Executing episode:43 with epsilon value:0.6670137476349562\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:44 with epsilon value:0.6604763516169062\n",
            "Executing episode:45 with epsilon value:0.6540040037760834\n",
            "Executing episode:46 with epsilon value:0.64759605687231\n",
            "Executing episode:47 with epsilon value:0.6412518701055556\n",
            "Executing episode:48 with epsilon value:0.6349708090518567\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:49 with epsilon value:0.6287522455998737\n",
            "Executing episode:50 with epsilon value:0.6225955578880794\n",
            "Executing episode:51 with epsilon value:0.616500130242572\n",
            "Executing episode:52 with epsilon value:0.6104653531155071\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:53 with epsilon value:0.6044906230241432\n",
            "Executing episode:54 with epsilon value:0.5985753424904925\n",
            "Executing episode:55 with epsilon value:0.5927189199815717\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:56 with epsilon value:0.5869207698502498\n",
            "Executing episode:57 with epsilon value:0.5811803122766818\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:58 with epsilon value:0.5754969732103268\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:59 with epsilon value:0.5698701843125418\n",
            "Executing episode:60 with epsilon value:0.564299382899748\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:61 with epsilon value:0.558784011887162\n",
            "Executing episode:62 with epsilon value:0.5533235197330861\n",
            "Executing episode:63 with epsilon value:0.5479173603837548\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:64 with epsilon value:0.5425649932187278\n",
            "Executing episode:65 with epsilon value:0.5372658829968282\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:66 with epsilon value:0.5320194998026181\n",
            "Executing episode:67 with epsilon value:0.5268253189934059\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:68 with epsilon value:0.5216828211467822\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:69 with epsilon value:0.516591492008677\n",
            "Executing episode:70 with epsilon value:0.5115508224419336\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:71 with epsilon value:0.5065603083753949\n",
            "Executing episode:72 with epsilon value:0.5016194507534953\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:73 with epsilon value:0.49672775548635545\n",
            "Executing episode:74 with epsilon value:0.491884733400372\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:75 with epsilon value:0.48708990018930043\n",
            "Executing episode:76 with epsilon value:0.48234277636582407\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:77 with epsilon value:0.47764288721360454\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:78 with epsilon value:0.47298976273981014\n",
            "Executing episode:79 with epsilon value:0.4683829376281158\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:80 with epsilon value:0.4638219511921713\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:81 with epsilon value:0.4593063473295323\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:82 with epsilon value:0.45483567447604933\n",
            "Executing episode:83 with epsilon value:0.4504094855607117\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:84 with epsilon value:0.44602733796093924\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:85 with epsilon value:0.4416887934583202\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:86 with epsilon value:0.43739341819478894\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:87 with epsilon value:0.43314078262923944\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:88 with epsilon value:0.4289304614945713\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:89 with epsilon value:0.42476203375516264\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:90 with epsilon value:0.42063508256476556\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:91 with epsilon value:0.41654919522482203\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:92 with epsilon value:0.4125039631431931\n",
            "Executing episode:93 with epsilon value:0.40849898179329963\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:94 with epsilon value:0.404533850673669\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:95 with epsilon value:0.40060817326788506\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:96 with epsilon value:0.3967215570049359\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:97 with epsilon value:0.3928736132199562\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:98 with epsilon value:0.38906395711536096\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:99 with epsilon value:0.38529220772236483\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:100 with epsilon value:0.3815579878628856\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:101 with epsilon value:0.37786092411182526\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:102 with epsilon value:0.3742006467597279\n",
            "Executing episode:103 with epsilon value:0.3705767897758081\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:104 with epsilon value:0.3669889907713475\n",
            "Executing episode:105 with epsilon value:0.36343689096345594\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:106 with epsilon value:0.35992013513919235\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:107 with epsilon value:0.35643837162004377\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:108 with epsilon value:0.3529912522267568\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:109 with epsilon value:0.34957843224451957\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:110 with epsilon value:0.34619957038848975\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:111 with epsilon value:0.34285432876966604\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:112 with epsilon value:0.3395423728610988\n",
            "Executing episode:113 with epsilon value:0.3362633714644372\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:114 with epsilon value:0.33301699667680906\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:115 with epsilon value:0.3298029238580304\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:116 with epsilon value:0.3266208315981408\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:117 with epsilon value:0.32347040168526264\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:118 with epsilon value:0.3203513190737793\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:119 with epsilon value:0.3172632718528302\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:120 with epsilon value:0.31420595121511996\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:121 with epsilon value:0.311179051426037\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:122 with epsilon value:0.3081822697930801\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:123 with epsilon value:0.3052153066355885\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:124 with epsilon value:0.30227786525477407\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:125 with epsilon value:0.29936965190405085\n",
            "Executing episode:126 with epsilon value:0.29649037575966014\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:127 with epsilon value:0.29363974889158817\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:128 with epsilon value:0.2908174862347727\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:129 with epsilon value:0.28802330556059597\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:130 with epsilon value:0.2852569274486622\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:131 with epsilon value:0.2825180752588548\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:132 with epsilon value:0.27980647510367246\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:133 with epsilon value:0.2771218558208399\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:134 with epsilon value:0.27446394894619186\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:135 with epsilon value:0.27183248868682575\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:136 with epsilon value:0.26922721189452276\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:137 with epsilon value:0.2666478580394326\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:138 with epsilon value:0.26409416918402034\n",
            "Executing episode:139 with epsilon value:0.26156588995727226\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:140 with epsilon value:0.2590627675291589\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:141 with epsilon value:0.2565845515853515\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:142 with epsilon value:0.2541309943021904\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:143 with epsilon value:0.25170185032190273\n",
            "Executing episode:144 with epsilon value:0.24929687672806608\n",
            "Executing episode:145 with epsilon value:0.246915833021317\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:146 with epsilon value:0.24455848109530054\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:147 with epsilon value:0.2422245852128597\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:148 with epsilon value:0.23991391198246126\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:149 with epsilon value:0.2376262303348566\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:150 with epsilon value:0.2353613114999746\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:151 with epsilon value:0.23311892898404435\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:152 with epsilon value:0.23089885854694553\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:153 with epsilon value:0.22870087817978443\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:154 with epsilon value:0.22652476808269262\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:155 with epsilon value:0.22437031064284702\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:156 with epsilon value:0.22223729041270818\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:157 with epsilon value:0.22012549408847562\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:158 with epsilon value:0.21803471048875708\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:159 with epsilon value:0.21596473053345028\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:160 with epsilon value:0.21391534722283462\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:161 with epsilon value:0.2118863556168713\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:162 with epsilon value:0.20987755281470882\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:163 with epsilon value:0.20788873793439305\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:164 with epsilon value:0.2059197120927785\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:165 with epsilon value:0.20397027838564025\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:166 with epsilon value:0.20204024186798297\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:167 with epsilon value:0.20012940953454655\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:168 with epsilon value:0.1982375903005053\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:169 with epsilon value:0.19636459498235934\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:170 with epsilon value:0.19451023627901587\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:171 with epsilon value:0.19267432875305937\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:172 with epsilon value:0.19085668881220733\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:173 with epsilon value:0.1890571346909509\n",
            "Executing episode:174 with epsilon value:0.1872754864323783\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:175 with epsilon value:0.18551156587017906\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:176 with epsilon value:0.1837651966108269\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:177 with epsilon value:0.1820362040159407\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:178 with epsilon value:0.18032441518482004\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:179 with epsilon value:0.17862965893715535\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:180 with epsilon value:0.17695176579590954\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:181 with epsilon value:0.1752905679703703\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:182 with epsilon value:0.17364589933937066\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:183 with epsilon value:0.172017595434677\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:184 with epsilon value:0.17040549342454195\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:185 with epsilon value:0.16880943209742102\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:186 with epsilon value:0.16722925184585147\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:187 with epsilon value:0.16566479465049133\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:188 with epsilon value:0.16411590406431734\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:189 with epsilon value:0.1625824251969801\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:190 with epsilon value:0.16106420469931504\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:191 with epsilon value:0.15956109074800714\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:192 with epsilon value:0.1580729330304087\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:193 with epsilon value:0.15659958272950783\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:194 with epsilon value:0.15514089250904667\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:195 with epsilon value:0.1536967164987875\n",
            "Executing episode:196 with epsilon value:0.1522669102799259\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:197 with epsilon value:0.15085133087064845\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:198 with epsilon value:0.14944983671183457\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:199 with epsilon value:0.14806228765290044\n",
            "Copying main network weights to the target network weights\n",
            "Executing episode:200 with epsilon value:0.1466885449377839\n",
            "Copying main network weights to the target network weights\n",
            "Average rewards per twenty episodes\n",
            "20 : 17.95\n",
            "40 : 15.299999999999999\n",
            "60 : 31.75\n",
            "80 : 55.0\n",
            "100 : 90.0\n",
            "120 : 85.0\n",
            "140 : 90.0\n",
            "160 : 90.0\n",
            "180 : 95.0\n",
            "200 : 95.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gaXTUl2K-oc"
      },
      "source": [
        "#### Interpreting the above result\n",
        "\n",
        "Initially it's taking 4-7 episodes before the weights of target net is updated i.e. 100 (value after which we want to update the weights of Target Network) time steps takes 4-7 episodes. As we reach the 70th episode we see that 100 time steps get covered in just 2-3 episodes and after 95th it just takes 1 episode which means that our pole is able to balance for more time steps which means it is learning.\n",
        "\n",
        "Increasing average reward per 20 episodes over the 200 episodes again shows us that our algorithm is learning over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRZA2S4XK-oc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}